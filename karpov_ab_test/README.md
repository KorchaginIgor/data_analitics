# Провести три исследования: Симуляция 10000 АА-тестов, Анализ данных АB-теста, Анализ теста между группами по метрике линеаризованных лайков.
## Данные
У нас есть данные АА-теста с '2022-05-17' по '2022-05-23' и АВ-теста с '2022-05-24' по '2022-05-30' о поведении пользователей в приложении
## Задачи
1. [тетрадка](https://github.com/KorchaginIgor/data_analitics/tree/main/karpov_ab_test/ab1) Нужно сделать симуляцию, как будто мы провели 10000 АА-тестов. На каждой итерации нам нужно сформировать подвыборки с повторением в 500 юзеров из 2 и 3 экспериментальной группы. Провести сравнение этих подвыборок t-testом.
2. [тетрадка](https://github.com/KorchaginIgor/data_analitics/tree/main/karpov_ab_test/ab2) Нужно проанализировать данные АB-теста различными методами и прийти к выводам об успешности этого теста.
3. [тетрадка](https://github.com/KorchaginIgor/data_analitics/tree/main/karpov_ab_test/ab3) Провести тесты по новой метрике линеаризованных лайков и сделать выводы об улучшении/ухудшении результатов или о более явных результатах.
## Используемые библиотеки
*Pandas, Hashlib, Python, Pandahouse, Swifter, Seaborn, Matplotlib, Scipy, Random, Numpy*
## Выводы
1. Доля p-values, равных или меньше 0,5, составляет чуть больше 7%. Само распределение p-values равномерное. Но есть некоторый выступ в районе нуля.
Из этого можно сделать вывод, что выборки довольно одинаковые. Система сплитования в целом работает. 

2. Почти все тесты показали, что выборки не одинаковые. Это можно увидеть по распределениям и квартилям. Эксперимент привел к тому, что для одних пользователей CTR стал меньше, а для других вырос.

Предположу причины такого результата:

- Разные группы пользователей по-разному реагируют на сам алгоритм. Например, одним может не нравится, что им алгоритм выдает не те посты, к которым они привыкли, им не нравятся изменения т.д., а другие лояльны к нашему алгоритму и изменения им подуше.
- Ошибка в алгоритме. Для одних он работал хорошо, для других работал с ошибкой в алгоритме. Эксперимент имеет хороший потенциал, потому что для тех у кого CTR упал алгоритм можно отключить. Тогда общий CTR вырастет, чего мы и добивались.

Рекомендации:

- учитывая сильное падение CTR для примерно половины тестовых пользователей, лучше остановить эксперимент.
- найти причины проблемы. Начать с изучения алгоритма.
- изучать 2 группы - у которых CTR вырос, и у которых упал. Возможно есть какие-то особенности.
- провести повторный тест после исправлений
- если повторный тест окажется успешным, то можно дать новый алгоритм всем пользователям.

3. Для групп 0 и 3 p-value t-теста и теста Манна-Уитни сильно меньше 0,05 на обычном CTR, то есть гипотеза о равенстве средних не подтверждается. Для линеаризованных лайков гипотеза о равенстве средних также не подтвердилась, p-value стал меньше в обоих тестах.
Для групп 1 и 2 p-value t-теста сильно больше 0,05, а для теста Манна-Уитни сильно меньше 0,05 на обычном CTR, то есть гипотеза о равенстве средних не подтверждается для теста Манна-Уитни и подтверждается для т-теста. Для линеаризованных лайков на обоих тестах гипотеза о равенстве средних также не подтвердилась, p-value стал меньше в т-тесте и больше в тесте Манна-Уитни.